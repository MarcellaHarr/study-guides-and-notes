# DSA I {.unnumbered}

```{r include=FALSE}
library(vembedr)
library(reticulate)
```

::: {.lead}

Data Structures and Algorithms I (C949)

:::

<br>

## Explains Algorithms (29%)


#### Characteristics of Algorithms

::: {.callout-note title="Names" collapse="true" appearance="simple" icon="false" style="border-left:none; border-right:none; border-top:none; border-bottom:none;"}

<br>

**Finiteness**

An algorithm must always have a finite number of steps before it ends. When the 
operation is finished, it must have a defined endpoint or output and not enter 
an endless loop.

**Definiteness**

An algorithm needs to have exact definitions for each step. Clear and 
straightforward directions ensure that every step is understood and can be 
taken easily.

**Input**

An algorithm requires one or more inputs. The values that are first supplied to 
the algorithm before its processing are known as inputs. These inputs come from 
a predetermined range of acceptable values.

**Output**

One or more outputs must be produced by an algorithm. The output is the outcome 
of the algorithm after every step has been completed. The relationship between 
the input and the result should be clear.

**Effectiveness**

An algorithm's stages must be sufficiently straightforward to be carried out in 
a finite time utilizing fundamental operations. With the resources at hand, 
every operation in the algorithm should be doable and practicable.

**Generality**

Rather than being limited to a single particular case, an algorithm should be 
able to solve a group of issues. It should offer a generic fix that manages a 
variety of inputs inside a predetermined range or domain.

:::

#### Factors of an Algorithm

::: {.callout-note title="Factors" collapse="true" appearance="simple" icon="false" style="border-left:none; border-right:none; border-top:none; border-bottom:none;"}

<br>

`**Modularity**`

This feature was perfectly designed for the algorithm if you are given a problem and break it down into small-small modules or small-small steps, which is a basic definition of an algorithm.

**Correctness**

An algorithm's correctness is defined as when the given inputs produce the desired output, indicating that the algorithm was designed correctly. An algorithm's analysis has been completed correctly.

**Maintainability**

It means that the algorithm should be designed in a straightforward, structured way so that when you redefine the algorithm, no significant changes are made to the algorithm.

**Functionality**

It takes into account various logical steps to solve a real-world problem.

**Robustness**

Robustness refers to an algorithm's ability to define your problem clearly.

**User-friendly**

If the algorithm is difficult to understand, the designer will not explain it to the programmer.

**Simplicity**

If an algorithm is simple, it is simple to understand.

`**Extensibility**`

Your algorithm should be extensible if another algorithm designer or programmer wants to use it.

<br>

:::

#### Types of Algorithms

<br>

::: {.panel-tabset}

## Type-1

<br>

```{r bruteVid, echo=FALSE}
embed_youtube(
  id = "BYWf6-tpQ4k",
  width = NULL,
  height = 315,
  ratio = "16by9",
  frameborder = 0,
  allowfullscreen = TRUE,
  query = NULL)
```

<br>

**Brute Force Algorithm**

A straightforward approach that exhaustively tries all possible solutions, 
suitable for small problem instances but may become impractical for larger 
ones due to its high time complexity.

---

```{r recursiveVid, echo=FALSE}
embed_youtube(
  id = "ivl5-snqul8",
  width = NULL,
  height = 315,
  ratio = "16by9",
  frameborder = 0,
  allowfullscreen = TRUE,
  query = NULL)
```

<br>

**Recursive Algorithm**

A method that breaks a problem into smaller, similar subproblems and repeatedly 
applies itself to solve them until reaching a base case, making it effective 
for tasks with recursive structures.

---

```{r encryptVid, echo=FALSE}
embed_youtube(
  id = "0TCY1rfpjxc",
  width = NULL,
  height = 315,
  ratio = "16by9",
  frameborder = 0,
  allowfullscreen = TRUE,
  query = NULL)
```

<br>

**Encryption Algorithm** 

Utilized to transform data into a secure, unreadable form using cryptographic 
techniques, ensuring confidentiality and privacy in digital communications and 
transactions.

## Type-2

<br>

```{r backtickVid, echo=FALSE}
embed_youtube(
  id = "RtpJOGvfo7E",
  width = NULL,
  height = 315,
  ratio = "16by9",
  frameborder = 0,
  allowfullscreen = TRUE,
  query = NULL)
```

<br>

**Backtracking Algorithm**

A trial-and-error technique used to explore potential solutions by undoing 
choices when they lead to an incorrect outcome, commonly employed in puzzles 
and optimization problems.

---

```{r searchVid, echo=FALSE}
embed_youtube(
  id = "ZpFqAoO4YPc",
  width = NULL,
  height = 315,
  ratio = "16by9",
  frameborder = 0,
  allowfullscreen = TRUE,
  query = NULL)
```

<br>

**Searching Algorithm**

Designed to find a specific target within a data set, enabling efficient 
retrieval of information from sorted or unsorted collections.

---

```{r sortVid, echo=FALSE}
embed_youtube(
  id = "rbbTd-gkajw",
  width = NULL,
  height = 315,
  ratio = "16by9",
  frameborder = 0,
  allowfullscreen = TRUE,
  query = NULL)
```

<br>

**Sorting Algorithm**

Aimed at arranging elements in a specific order, like numerical or alphabetical, 
to enhance data organization and retrieval.

## Type-3

<br>

```{r hashVid_1, echo=FALSE}
embed_youtube(
  id = "WEILxTBDy0Y",
  width = NULL,
  height = 315,
  ratio = "16by9",
  frameborder = 0,
  allowfullscreen = TRUE,
  query = NULL)
```

```{r hashVid_2, echo=FALSE}
embed_youtube(
  id = "FsfRsGFHuv4",
  width = NULL,
  height = 315,
  ratio = "16by9",
  frameborder = 0,
  allowfullscreen = TRUE,
  query = NULL)
```

<br>

**Hashing Algorithm** 

Converts data into a fixed-size hash value, enabling rapid data access and 
retrieval in hash tables, commonly used in databases and password storage.

---

```{r divideVid, echo=FALSE}
embed_youtube(
  id = "ib4BHvr5-Ao",
  width = NULL,
  height = 315,
  ratio = "16by9",
  frameborder = 0,
  allowfullscreen = TRUE,
  query = NULL)
```

<br>

**Divide & Conquer Algorithm**

Breaks a complex problem into smaller subproblems, solves them independently, 
and then combines their solutions to address the original problem effectively.

---

```{r greedyVid, echo=FALSE}
embed_youtube(
  id = "hDDrIw_DSto",
  width = NULL,
  height = 315,
  ratio = "16by9",
  frameborder = 0,
  allowfullscreen = TRUE,
  query = NULL)
```

<br>

**Greedy Algorithm**

Makes locally optimal choices at each step in the hope of finding a global 
optimum, useful for optimization problems but may not always lead to the best 
solution.

## Type-4

<br>

```{r dynamicVid, echo=FALSE}
embed_youtube(
  id = "vYquumk4nWw",
  width = NULL,
  height = 315,
  ratio = "16by9",
  frameborder = 0,
  allowfullscreen = TRUE,
  query = NULL)
```

<br>

**Dynamic Programming Algorithm**

Stores and reuses intermediate results to avoid redundant computations, 
enhancing the efficiency of solving complex problems.

---

```{r randomVid, echo=FALSE}
embed_youtube(
  id = "8t9RaIQzp_o",
  width = NULL,
  height = 315,
  ratio = "16by9",
  frameborder = 0,
  allowfullscreen = TRUE,
  query = NULL)
```

<br>

**Randomized Algorithm**

Utilizes randomness in its steps to achieve a solution, often used in situations 
where an approximate or probabilistic answer suffices.

:::

<br>

#### Recursive Algorithms

<br>

##### Algorithms

Recursive algorithms are a fundamental concept in computer science, particularly 
in the study of data structures and algorithms. A recursive algorithm is one 
that solves a problem by breaking it down into smaller instances of the same 
problem, which it then solves in the same way. This process continues until the 
problem is reduced to a base case, which is solved directly without further 
recursion.

<br>

::: {.lead}
##### Key Concepts 
:::

::: {.panel-tabset}

## Base Case

This is the condition under which the recursion stops. It represents the simplest 
instance of the problem, which can be solved directly without further recursion.

## Recursive Case

This is the part of the algorithm that breaks the problem down into smaller 
instances of the same problem and then calls the algorithm recursively on these 
smaller instances.

## Stack

Each recursive call is placed on the system call stack. When the base case is 
reached, the stack begins to unwind as each instance of the function returns 
its result.

<br>

:::

<br>

::: {.lead}
##### Factorial Calculation
:::

The factorial of a number n (denoted as n!) is a classic example of a recursive 
algorithm. The factorial is defined as:

  - O! = 1 *(Base Case)*
  - N! = n * (n-1)! For n > O *(Recursive Case)*

::: {.panel-tabset}

## Code

```{python factorialCode}
def factorial(n):
    if n == 0:  # Base Case
        return 1
    else:  # Recursive Case
        return n * factorial(n - 1)
```

## Logic

**How It Works:**

  - Base Case: When n is 0, the function returns 1.
  - Recursive Case: For any other value of n, the function calls itself with n−1 and multiplies the result by n.
  
For example, calling `factorial(3)` would work as follows:

  - `factorial(3)` calls `factorial(2)`
  - `factorial(2)` calls `factorial(1)`
  - `factorial(1)` calls `factorial(0)`
  - `factorial(0)` returns 1, then:
  - `factorial(1)` returns 1 * 1 = 1
  - `factorial(2)` returns 2 * 1 = 2
  - `factorial(3)` returns 3 * 2 = 6

## Pros/Cons

**Advantages of Recursion**

  - **Simplicity:** Recursive solutions are often more elegant and easier to understand than their iterative counterparts.
  - **Direct Translation:** Some problems are naturally recursive, like tree traversals, making recursion the most straightforward approach.

**Disadvantages of Recursion**

  - **Performance:** Recursive algorithms can be less efficient due to the overhead of multiple function calls and potential stack overflow issues for deep recursion.
  - **Memory Usage:** Recursion can consume more memory because each function call adds a new frame to the call stack.

## Usage

**When to Use Recursion**
  - When a problem can naturally be divided into similar sub-problems *(e.g., tree traversal, searching algorithms like binary search)*.
  - When the recursive solution is significantly simpler or more intuitive than an iterative one.

<br>

:::

<br>

::: {.lead}
##### Linear & Binary Search
:::

Linear search and binary search are two fundamental algorithms used to search 
for an element in a collection, like an array or a list. However, they differ 
significantly in how they approach the search and their efficiency.

<br>

::: {.lead}
###### *Linear Search*
:::

::: {.panel-tabset}

## Concept

- Linear search is the simplest search algorithm.
- It works by sequentially checking each element of the array or list until the target element is found or the end of the collection is reached.

## Steps

**Algorithm:**

  1. Start from the first element of the array.
  2. Compare the current element with the target element.
  3. If they match, return the index of the element.
  4. If they don't match, move to the next element and repeat the process.
  5. If the target element is not found by the end of the array, return a "not found" indication.

**Time Complexity:** $O(n)$, where n is the number of elements in the array. This is 
because in the worst case, the algorithm may need to check every element in the array.

## Usage

**When to Use:**

  - When the array or list is small.
  - When the array is unsorted.
  - When simplicity is more important than performance.
  

```{python linearCode}
def linear_search(arr, target):
    for i in range(len(arr)):
        if arr[i] == target:
            return i
    return -1  # Return -1 if the element is not found
```

<br>

:::

<br>

::: {.lead}
###### *Binary Search*
:::

::: {.panel-tabset}

## Concept

  - Binary search is much more efficient than linear search but requires the array or list to be sorted.
  - It works by repeatedly dividing the search interval in half. If the target value is less than the middle element, the search continues in the left half, otherwise in the right half.

## Steps

**Algorithm:**

  1. Start with two pointers, one at the beginning (low) and one at the end (high) of the sorted array.
  2. Find the middle element of the current interval.
  3. Compare the middle element with the target:
      - If they match, return the index of the middle element.
      - If the target is less than the middle element, repeat the search on the left half.
      - If the target is greater, repeat the search on the right half.
  4. If the interval becomes invalid (low > high), return a "not found" indication.


**Time Complexity:** $\text{O(log⁡ n)}$, where n is the number of elements in the array. This logarithmic time complexity makes binary search significantly faster than linear search for large data sets.

## Usage

**When to Use:**

  - When the array or list is sorted.
  - When the array is large and efficiency is crucial.
  
```{python binarySearchCode}
def binary_search(arr, target):
    low = 0
    high = len(arr) - 1

    while low <= high:
        mid = (low + high) // 2
        if arr[mid] == target:
            return mid
        elif arr[mid] < target:
            low = mid + 1
        else:
            high = mid - 1

    return -1  # Return -1 if the element is not found
```

## Compare

**Comparison**

  - **Efficiency:** Binary search is faster than linear search, especially for large data sets, but it requires the array to be sorted.
  - **Simplicity:** Linear search is simpler to implement and doesn't require the array to be sorted, making it more versatile for smaller or unsorted data sets.
  - **Use Cases:**
      - **Linear Search:** Suitable for small or unsorted collections where the simplicity of the algorithm outweighs the need for speed.
      - **Binary Search:** Ideal for large, sorted collections where performance is a priority.

<br>

:::

<br>

::: {.lead}
###### *Step-by-Step Guide*
:::

::: {.panel-tabset}

## Arrays

Figuring out the array elements that correspond to the mid-values in the first 
and second iterations of A binary search

$\text{ arr = {45, 77, 89, 90, 94, 99, 100} }$ and $\text{key = 100}$

## Setup

  - The array `arr` is `{45, 77, 89, 90, 94, 99, 100}`.
  - The `key` to find is `100`.
  - Initialize two pointers: `low` (start of the array) and `high` (end of the array).

## 1st Iteration

  - Calculate the middle index `mid` using the formula: `mid = (low + high) / 2`
  - Check the value at `arr[mid]`.
  - Compare `arr[mid]` with the `key`:
  
      - If `arr[mid]` is less than `key`, update `low` to `mid` + 1.
      - If `arr[mid]` is greater than `key`, update `high` to `mid` - 1.
      - If `arr[mid]` is equal to `key`, you have found the `key` (though you won't need a second iteration in this case).

## 2nd Iteration

  - Repeat the calculation for `mid` with the updated `low` and `high` values.
  - Again, compare `arr[mid]` with the `key` and update `low` or `high` accordingly.

<br>

:::

<br>

#### Searching Algorithms

<br>

::: {.lead}
##### Linear Search
:::

<br>

::: {.panel-tabset}

## Logic

  - **Concept:** As discussed earlier, linear search involves checking each element in a list or array sequentially until the target element is found or the end of the collection is reached.
  - **Time Complexity:** $O(n)$, where n is the number of elements.

## Usage

  - **Use Case:** Best used when the list is small or unsorted.
  
  - **Distinct Characteristics:**
  
      - Simple, sequential search.
      - Checks each element one by one.
      - Works on both sorted and unsorted data.

## States

  - **Linear Search Worst, Average and Best**
  
      - `**Best Case:**` $O(1)$ — The target element is the first element.
      - `**Average Case:**` $O(n)$ — The target element is somewhere in the middle or not in the array.
      - `**Worst Case:**` $O(n)$ — The target element is the last element or not present.

<br>

:::

<br>

::: {.lead}
##### Binary Search
:::

<br>

::: {.panel-tabset}

## Logic

  - **Concept:** Binary search operates on a sorted list. It repeatedly divides the search interval in half until the target element is found or the interval is empty.
  - **Time Complexity:** $\text{O(log⁡ n)}$

## Usage

  - **Use Case:** Ideal for large, sorted datasets.

  - **Distinct Characteristics:**

      - `Requires a sorted array.`
      - Divides the search interval in half repeatedly.
      - Efficient, logarithmic time complexity.

## States

  - **Binary Search Worst, Average and Best**

      - **Best Case:** $O(1)$ — The target element is the middle element.
      - **Average Case:** $\text{O(log⁡ n)}$ — The target element is not immediately found but within the sorted array.
      - **Worst Case:** $\text{O(log⁡ n)}$ — The target element is at the extreme ends or not present.

<br>

:::

<br>

::: {.lead}
##### Interpolation Search
:::

<br>

::: {.panel-tabset}

## Logic

  - **Concept:** Similar to binary search but works on uniformly distributed data. It estimates the position of the target element based on the value.
  - **Time Complexity:** O(log ⁡log ⁡n) in the best case, $O(n)$ in the worst case.

## Usage

  - **Use Case:** Effective when the data is uniformly distributed.

## States

  - Interpolation Search Worst, Average and Best
      - **Best Case:** $O(1)$ — The target element is exactly where the interpolation suggests.
      - **Average Case:** $\text{O(log ⁡log⁡ n)}$ — Uniformly distributed data.
      - **Worst Case:** $O(n)$ — Highly skewed data distribution or worst interpolation.

<br>

:::

<br>

::: {.lead}
##### DFS/BFS
:::

<br>

::: {.panel-tabset}

## Logic

  - **Concept:** Used primarily in graph and tree data structures. **Depth-First Search (DFS)** explores as far as possible along one branch before backtracking, while **Breadth-First Search (BFS)** explores all neighbors at the present depth before moving on to nodes at the next depth level.
  - **Time Complexity:** $\text{O(V + E)}$, where V is the number of vertices and E is the number of edges.

## Usage

  - **Use Case:** Useful for searching nodes in graphs and trees.

## States

  - **(DFS)**

      - **Best Case:** $O(1)$ — The target node is found immediately.
      - **Average Case:** $\text{O(V + E)}$— Typically when all nodes and edges must be explored.
      - **Worst Case:** $\text{O(V + E)}$ — The target node is the last one discovered.

  - **(BFS)**

      - **Best Case:** $O(1)$ — The target node is the root or the first node checked.
      - **Average Case:** $\text{O(V + E)}$ — All nodes and edges need to be explored.
      - **Worst Case:** $\text{O(V + E)}$ — The target node is the last one explored.

<br>

:::

<br>

#### Sorting Algorithms

Sorting algorithms organize data in a particular order *(usually ascending or descending)*. 
This makes searching and other operations more efficient.

<br>

::: {.lead}
##### Bubble Sort
:::

<br>

::: {.panel-tabset}

## Usage

  - **Use Case:** Simple but inefficient for large datasets. Best used for educational purposes or small lists.

## Descriptor

  - **Distinct Characteristics:**

      - Repeatedly swaps adjacent elements if they are in the wrong order.
      - Simple, but inefficient for large datasets.
      - “Bubbles” the largest element to the end of the list.

## States

  - Bubble Sort Worst, Average and Best
      - **Best Case:** $O(n)$ — The array is already sorted *(with an optimized version that stops early)*.
      - **Average Case:** $O(n^2)$ — Average case with random elements.
      - **Worst Case:** $O(n^2)$ — The array is sorted in reverse order.

## Tips

  - **Bubble:** Look for something that swaps so the result can “bubble” to the top. (Swap, Exchange)

<br>

:::

<br>

::: {.lead}
##### Selection Sort
:::

<br>

::: {.panel-tabset}

## Usage

  - **Use Case:** Inefficient for large lists, but useful when memory writes are more expensive than comparisons.

## Descriptor

  - **Distinct Characteristics:**

      - Finds the minimum element and swaps it with the first unsorted element.
      - Reduces the problem size by one in each iteration.
      - Always performs $O(n^2)$ comparisons, regardless of input.

## States

  - Selection Sort Worst, Average and Best

      - **Best Case:** $O(n^2)$ — Selection sort does not improve with better input, always $O(n^2)$.
      - **Average Case:** $O(n^2)$ — Average case with random elements.
      - **Worst Case:** $O(n^2)$ — Selection sort is insensitive to input order.

## Tips

  - **Selection:** Look for code that repeatedly finds the minimum *(or maximum)* element and moves it to the beginning *(or end)* of the list. *(Select minimum, Swap with start)*

<br>

:::

<br>

::: {.lead}
##### Insertion Sort
:::

<br>

::: {.panel-tabset}

## Usage

  - **Use Case:** Good for small or nearly sorted lists.

## Descriptor

  - **Distinct Characteristics:**

      - Builds a sorted list one element at a time.
      - Efficient for small or nearly sorted datasets.
      - Shifts elements to make space for the current element.

## States

  - **Insertion Sort Worst, Average and Best**

      - **Best Case:** $O(n)$ — The array is already sorted.
      - **Average Case:** $O(n^2)$ — Average case with random elements.
      - **Worst Case:** $O(n^2)$ — The array is sorted in reverse order.

## Tips

  - **Insertion:** Look for code that builds a sorted portion of the list one element at a time by inserting each new element into its correct position within the already-sorted part. *(Insert, Shift Element)*

<br>

:::

<br>

::: {.lead}
##### Merge Sort
:::

<br>

::: {.panel-tabset}

## Usage

  - **Use Case:** Efficient and stable; good for large datasets.

## Descriptor

  - **Distinct Characteristics:**

      - Divides the list into halves, sorts each half, and then merges them.
      - Stable and efficient for large datasets.
      - Requires additional space for merging.

## States

  - **Merge Sort Worst, Average and Best**

      - **Best Case:** $\text{O(n ⁡log⁡ n)}$ — Merge sort’s time complexity is the same in all cases.
      - **Average Case:** $\text{O(n ⁡log⁡ n)}$.
      - **Worst Case:** $\text{O(n ⁡log⁡ n)}$.

## Tips

  - **Merge:** Look for something that continually splits a list in half. *(Merge, Split)*

<br>

:::

<br>

::: {.lead}
##### Quicksort
:::

<br>

::: {.panel-tabset}

## Usage

  - **Use Case:** Often faster in practice than merge sort but less stable.

## Descriptor

  - **Distinct Characteristics:**

      - Selects a *“pivot”* element and partitions the array around it.
      - Recursively sorts the partitions.
      - Efficient, but can degrade to $O(n^2)$ if poor pivot selection occurs.

## States

  - **Quicksort Worst, Average and Best**

      - **Best Case:** $\text{O(n ⁡log⁡ n)}$ — The pivot splits the array into two nearly equal halves.
      - **Average Case:** $\text{O(n ⁡log⁡ n)}$ — Average case with random pivots.
      - **Worst Case:** $O(n^2)$ — The pivot is always the smallest or largest element, leading to unbalanced partitions.

## Tips

  - **Quicksort:** Look for the keywords *“pivot”* and/or *“split”*. *(Pivot, Split)*

<br>

:::

<br>

::: {.lead}
##### Heap Sort
:::

<br>

::: {.panel-tabset}

## Usage

  - **Use Case:** Useful when memory usage is a concern as it’s an in-place algorithm.

## Descriptor

  - **Distinct Characteristics:**

      - Utilizes a binary heap data structure.
      - Builds a max-heap and repeatedly extracts the maximum element.
      - Efficient and in-place, but not stable.

## States

  - **Heap Sort Worst, Average and Best**

      - **Best Case:** $\text{O(n ⁡log⁡ n)}$ — Heap sort’s time complexity is the same in all cases.
      - **Average Case:** $\text{O(n ⁡log⁡ n)}$.
      - **Worst Case:** $\text{O(n ⁡log⁡ n)}$.

## Tips

  - **Heap Sort:** Look for code that uses a heap data structure to repeatedly extract the maximum *(or minimum)* element and rebuilds the heap. *(Heapify, Extract Max, Build Heap)*

<br>

:::

<br>

::: {.lead}
##### Counting Sort
:::

<br>

::: {.panel-tabset}

## Usage

  - Use Case: Efficient for sorting integers or other items with a small range of possible values.

## Descriptor

  - **Distinct Characteristics:**

      - Non-comparative sorting.
      - Counts occurrences of each element and uses this information to place elements.
      - Efficient for small ranges of integers.

## States

  - **Counting Sort Worst, Average and Best**

      - **Best Case:** $\text{O(n ⁡+ k) - k}$ is the range of the input.
      - **Average Case:** $\text{O(n ⁡+ k)}$.
      - **Worst Case:** $\text{O(n ⁡+ k)}$.

<br>

:::

<br>

::: {.lead}
##### Radix Sort
:::

<br>

::: {.panel-tabset}

## Usage

  - **Use Case:** Effective for sorting large numbers or strings with a fixed length.

## Descriptor

  - **Distinct Characteristics:**

      - Sorts numbers by processing individual digits.
      - Non-comparative, stable, and efficient for specific data types.
      - Often combined with counting sort.

## States

  - **Radix Sort Worst, Average and Best**

      - **Best Case:** $\text{O(n ⁡* k)}$ — k is the number of digits in the largest number.
      - **Average Case:** $\text{O(n ⁡* k)}$.
      - **Worst Case:** $\text{O(n ⁡* k)}$.

## Tips

  - **Radix Sort:** Look for code that sorts numbers based on their individual digits, starting from the **least significant digit (LSD)** or the most **significant digit (MSD)**. *(Count, Frequency, Sum)*

<br>

:::

<br>

::: {.lead}
##### Bucket Sort
:::

<br>

::: {.panel-tabset}

## Usage

  - **Use Case:** Good for uniformly distributed data.

## Descriptor

  - **Distinct Characteristics:**
  
      - Distributes elements into buckets and sorts each bucket individually.
      - Efficient when the input is uniformly distributed.
      - Often combined with another sorting algorithm like insertion sort.

## States

  - **Bucket Sort Worst, Average and Best**

      - **Best Case:** $\text{O(n + k)}$ — k is the number of buckets; assumes uniform distribution.
      - **Average Case:** $\text{O(n + k)}$.
      - **Worst Case:** $O(n^2)$ — All elements end up in one bucket *(degenerate case)*.

## Tips

  - **Bucket:** Look for something that distributes the values into “buckets” where they are individually sorted. *(Bucket)*

<br>

:::

<br>

::: {.lead}
##### Shell Sort
:::

<br>

::: {.panel-tabset}

## Descriptor

  - **Distinct Characteristics:**

      - Generalization of insertion sort with a gap sequence.
      - Sorts elements far apart and gradually reduces the gap.
      - Efficient for medium-sized datasets.
      - Time Complexity: Depends on the gap sequence; commonly $\text{O(n3/2)}$.

## States

  - **Shell Sort Worst, Average and Best**

      - **Best Case:** $\text{O(n log n)}$ — Occurs when the array is already sorted or nearly sorted, especially when using a good gap sequence like the Knuth sequence.
      - **Average Case:** $O(n^(3/2))$ or $O(n^1.5)$ — Highly dependent on the gap sequence used. With commonly used sequences like the Knuth sequence, the average-case complexity is approximately $O(n^1.5)$.
      - **Worst Case:** ${O(n^2)}$ — Can degrade to ${O(n^2)}$, particularly with poorly chosen gap sequences like the original Shell sequence *(where the gaps are halved each time)*

## Tips

  - **Shell Sort:** Look for code that sorts elements at specific intervals and gradually reduces the interval until it performs a final insertion sort. *(Gap, Interval)*

<br>

:::

<br>

---

::: {.lead}
###### **Summary**
:::

<br>

::: {.callout-note title="Searching & Sorting Algorithms" collapse="true" appearance="simple" icon="false" style="border-left:none; border-right:none; border-top:none; border-bottom:none;"}

  - **Linear Search:** Simple, sequential; $O(n)$.
  - **Binary Search:** Sorted data, divide and conquer; $\text{O(log n)}$.
  - **Bubble Sort:** Swaps, bubbles up; $O(n^2)$.
  - **Selection Sort:** Finds minimum, swaps; $O(n^2)$.
  - **Insertion Sort:** Builds sorted list, shifts; $O(n^2)$, $O(n)$ best case.
  - **Merge Sort:** Divide and conquer, merge; $\text{O(n log n)}$.
  - **Quick Sort:** Pivot, partition; $\text{O(n log n)}$ average, $O(n^2)$ worst case.
  - **Heap Sort:** Max-heap, extract max; $\text{O(n log n)}$.
  - **Counting Sort:** Counts occurrences, non-comparative; $\text{O(n + k)}$.
  - **Radix Sort:** Sorts by digits, non-comparative; $O(nk)$.
  - **Bucket Sort:** Distributes into buckets, sorts; $\text{O(n + k)}$.
  - **Shell Sort:** Gap sequence, insertion-like; $O(n^3/2)$.

:::

::: {.callout-note title="Key Observations" collapse="true" appearance="simple" icon="false" style="border-left:none; border-right:none; border-top:none; border-bottom:none;"}

  - **Bubble Sort, Selection Sort, and Insertion Sort:** These are simple but inefficient for large datasets, especially in the worst case.
  - **Merge Sort and Heap Sort:** Stable and consistent in performance, regardless of the input.
  - **Quick Sort:** Very efficient on average but can degrade to $O(n^2)$  in the worst case without proper pivot selection.
  - **Counting Sort, Radix Sort, and Bucket Sort:** Efficient for specific types of data *(e.g., integers within a fixed range)* but less versatile.

:::

::: {.callout-note title="Choosing the Right Algorithm" collapse="true" appearance="simple" icon="false" style="border-left:none; border-right:none; border-top:none; border-bottom:none;"}

  - **Small datasets:** Simpler algorithms like bubble sort, selection sort, or insertion sort might suffice.
  - **Large datasets:** More efficient algorithms like merge sort, quick sort, or heap sort are preferred.
  - **Sorted data:** Algorithms like insertion sort can be very efficient.
  - **Special conditions:** Use counting sort, radix sort, or bucket sort if the data is within a certain range or has other specific properties.

<br>

:::

---

<br>

#### Big O Notation

**What is Big O Notation?**

  - **Big O Notation:** It provides an upper bound on the time or space complexity of an algorithm, representing the worst-case scenario. It’s a way to describe the efficiency of an algorithm as the input size grows towards infinity.

**Why Use Big O Notation?**

  - **Comparing Algorithms:** It allows us to compare the efficiency of different algorithms independently of hardware or other environmental factors.
  - **Scalability:** It helps us understand how an algorithm will perform as the size of the input data grows.

<br>

::: {.lead}
##### O Notations
:::

::: {.callout-note title="Common Big Os" collapse="true" appearance="simple" icon="false" style="border-left:none; border-right:none; border-top:none; border-bottom:none;"}

  1. **`O(1) - Constant Time`:**
      - **Description:** The algorithm takes the same amount of time to execute regardless of the size of the input.
      - **Example:** Accessing an element in an array by index.
      - **Efficiency:** Excellent.

  2. **`O(log n) - Logarithmic Time`:**
      - **Description:** The runtime increases logarithmically as the input size increases. Typically occurs in algorithms that halve the problem size at each step, like binary search.
      - **Example:** Binary search.
      - **Efficiency:** Very good for large inputs.

  3. **`O(n) - Linear Time`:**
      - **Description:** The runtime increases linearly with the size of the input. If you double the input size, the runtime also doubles.
      - **Example:** Linear search, iterating through a list.
      - **Efficiency:** Reasonable for moderate to large inputs.

  4. **`O(n log n) - Log-Linear Time`:**
      - **Description:** The runtime increases more than linearly but less than quadratically. Common in efficient sorting algorithms like merge sort and quicksort.
      - **Example:** Merge sort, quicksort, heap sort.
      - **Efficiency:** Efficient for large inputs.

  5. **`O(n^2) - Quadratic Time`:**
      - **Description:** The runtime increases quadratically with the size of the input. If you double the input size, the runtime quadruples.
      - **Example:** Bubble sort, insertion sort, selection sort *(for unsorted arrays)*.
      - **Efficiency:** Poor for large inputs.

  6. **`O(2n) - Exponential Time`:**
      - **Description:** The runtime doubles with each additional element in the input. Common in algorithms that solve problems by brute force or explore all possible solutions.
      - **Example:** Recursive algorithms for the Fibonacci sequence, certain dynamic programming problems.
      - **Efficiency:** Very poor, impractical for large inputs.

  7. **`O(n!) - Factorial Time`:**
      - **Description:** The runtime increases factorially with the size of the input. Common in algorithms that generate all permutations of an input set.
      - **Example:** Traveling salesman problem via brute force.
      - **Efficiency:** Extremely poor, infeasible for even moderate input sizes.

:::

::: {.callout-note title="Time Complexity" collapse="true" appearance="simple" icon="false" style="border-left:none; border-right:none; border-top:none; border-bottom:none;"}

**Best Case, Worst Case, and Average Case**

  - **Best Case:** The scenario where the algorithm performs the minimum possible number of operations. It’s often less relevant because it’s optimistic.
      - **Example:** For linear search, the best case is $O(1)$, where the target element is the first one in the array.

  - **Worst Case:** The scenario where the algorithm performs the maximum possible number of operations. Big O notation typically describes the worst-case complexity.
      - **Example:** For linear search, the worst case is $O(n)$, where the target element is the last one in the array or isn’t present at all.

  - **Average Case:** The scenario that represents the expected number of operations for a typical input. It’s more complex to calculate because it depends on the distribution of inputs.
      - **Example:** For linear search, the average case is $O(n/2)$, but in Big O notation, we simplify this to $O(n)$.

:::

::: {.callout-note title="Calculation Rules" collapse="true" appearance="simple" icon="false" style="border-left:none; border-right:none; border-top:none; border-bottom:none;"}

**Ignore Constants:**

  - **Rule:** In Big O notation, constant factors are ignored.
  - **Why:** Big O notation focuses on the growth rate as the input size $(n)$ increases, so a constant multiplier doesn't affect the growth rate.
  - **Example:** $O(2n)$ simplifies to $O(n)$

**Focus on the Dominant Term:**

  - **Rule:** Only the term with the highest growth rate is considered.
  - **Why:** As n becomes large, the term with the highest growth rate will dominate the others.
  - **Example:** $O(n2+n)$ simplifies to $O(n2)$

**Drop Lower Order Terms:**

  - **Rule:** Lower-order terms are ignored because they become insignificant as n grows.
  - **Why:** Similar to focusing on the dominant term, lower-order terms have a negligible impact on large inputs.
  - **Example:** $O(n2 + n + log n)$  simplifies to $O(n2)$

**Multiplicative Constants Can Be Ignored:**
  - **Rule:** Coefficients that multiply variables *(e.g., 2n, 3n^2)* are ignored.
  - **Why:** Like constants, they don't change the growth rate.
  - **Example:** $O(3n2)$ simplifies to $O(n2)$

**Additive Constants Can Be Ignored:**

  - **Rule:** Constant terms that don't depend on n are ignored.
  - **Why:** They don't affect the overall growth rate as n increases.
  - **Example:** $O(n+10)$ simplifies to $O(n)$

**Logarithms with Different Bases:**

  - **Rule:** Logarithms with different bases can be considered equivalent in Big O notation.
  - **Why:** Changing the base of a logarithm only introduces a constant factor, which is ignored in Big O notation.
  - **Example:** $\text{O(log⁡2 n)}$ simplifies to $\text{O(log⁡ n)}$

**Non-Dominant Polynomial Terms:**

  - **Rule:** In polynomials, only the highest degree term is considered.
  - **Why:** As n grows large, the highest degree term will dominate.
  - **Example:** $\text{O(5n3 + 2n2 + 7)}$ simplifies to $O(n3)$

**Exponential Growth:**

  - **Rule:** Exponential growth functions dominate polynomial functions.
  - **Why:** Exponential functions grow much faster than polynomial functions as n increases.
  - **Example:** $\text{O(2n + n3)}$ simplifies to $O(2n)$

**Nested Loops:**

  - **Rule:** The time complexity of nested loops is the product of the complexities of each loop.
  - **Why:** Each loop iterates based on the input size, so their combined effect is multiplicative.
  - **Example:** A loop inside another loop both running n times results in $\text{O(n * n) = O(n2)}$

**Sequential Statements:**

  - **Rule:** If two independent statements *(or loops)* are executed sequentially, their time complexities are added.
  - **Why:** Sequential operations don't multiply time complexity, but rather add up.
  - **Example:** Two loops each running n times sequentially result in $\text{O(n + n) = O(n)}$

<br>

:::

<br>

## Determines Data Structure Impact - (31%)

#### Implementation of Data Structures

<br>

::: {.lead}
##### Data Types
:::

<br>

::: {.panel-tabset}

## Definition

**What is a Data Type?**

**Definition:** A data type is a classification that specifies the type of data 
that a variable can hold in programming. It defines the operations that can be 
performed on the data and the way the data is stored in memory.

## Examples

  - **Primitive Data Types:** These are the basic building blocks of data in a programming language.
  
      - Integer *(e.g., `int` in C, Java)*: Holds whole numbers.
      - Floating Point *(e.g., `float`, `double`)*: Holds numbers with fractional parts.
      - Character *(e.g., `char`)*: Holds a single character.
      - Boolean *(e.g., `bool`)*: Holds a `true` or `false` value.
      
  - **Composite Data Types:** These are constructed from primitive types.
  
      - Arrays: A collection of elements of the same data type.
      - Structures *(e.g., `struct` in C)*: A collection of different data types.
      
  - **User-Defined Data Types:** Created by the user, typically by combining primitive data types.
  
      - Enumerations `(enum)`, Classes, etc.

## Extra

**Enumeration:**

  - Enumeration, often referred to as *"enum,"* is a data type in programming that allows a variable to be a set of predefined constants. 
  - These constants are typically related and represent a set of possible values that a variable of the enumeration type can hold. 
  - Enums improve code readability, make it easier to manage sets of related values, and reduce errors by limiting the values a variable can take.

**Key Concepts of Enumeration:**

  1. **Definition:** An enumeration is defined using the enum keyword *(syntax can vary by language)*. It consists of a set of named constants, each representing a unique value.
  2. **Values:** The values in an enum are usually integers by default, starting from 0, but they can be assigned specific values as needed.
  3. **Usage:** Enums are commonly used when a variable can only take one out of a small set of possible values, like days of the week, directions, states, etc.

**Example in Different Languages:**

C/C++

```{r cEnum, engine='c', results='hide'}
#| eval: false

enum Direction {
    NORTH,
    EAST,
    SOUTH,
    WEST
};

Direction dir = NORTH;
```

Java

```{r javaEnum, engine='js', results='hide'}
#| eval: false

public enum Direction {
    NORTH,
    EAST,
    SOUTH,
    WEST
}

Direction dir = Direction.NORTH;
```

Python (using enum module)

```{python pythonEnum}
from enum import Enum

class Direction(Enum):
    NORTH = 1
    EAST = 2
    SOUTH = 3
    WEST = 4

dir = Direction.NORTH
```

**Advantages of Using Enums:**
  - Readability: Code is easier to read and understand.
  - Maintainability: Easier to update and maintain related values.
  - Type Safety: Prevents assigning invalid values to variables of the enum type.

Enums are useful in scenarios where a variable should only be allowed to take one out of a small set of specific values, helping prevent errors and making the code clearer and more reliable.

**Key Characteristics:**
  - **Memory Allocation:** Data types determine how much memory is allocated for storing the data.
  - **Operations:** Each data type supports a set of operations, like arithmetic operations for integers or concatenation for strings.

<br>

:::

<br>

::: {.lead}
##### Data Structures
:::

<br>

::: {.panel-tabset}

## Definition

**What is a Data Structure?**

  - **Definition:** A data structure is a specific way of organizing and storing data in a computer so that it can be accessed and modified efficiently. Data structures use data types as their underlying foundation.

## Examples

  - **Arrays:** A collection of elements stored in contiguous memory locations.
  - **Linked Lists:** A series of connected nodes, where each node contains data and a reference to the next node.
  - **Stacks:** A collection of elements with Last-In-First-Out (LIFO) access.
  - **Queues:** A collection of elements with First-In-First-Out (FIFO) access.
  - **Trees:** A hierarchical structure with a root element and sub-elements called nodes.
  - **Graphs:** A collection of nodes *(vertices)* connected by edges.
  - **Hash Tables:** A data structure that maps keys to values for efficient lookup.

<br>

:::

<br>

::: {.lead}
##### Abstract Data Types (ADTs)
:::

<br>

::: {.panel-tabset}

## Definition

  - **Definition:** An abstract data type (ADT) is a theoretical model of a data structure that defines the behavior from the user's point of view, without specifying the underlying implementation. It specifies the operations that can be performed and the expected behavior but not how these operations are carried out.

## Examples

  - **List ADT:** Operations include insertion, deletion, and access by index.
  - **Stack ADT:** Operations include `push`, `pop`, and `peek`.
  - **Queue ADT:** Operations include `enqueue` and `dequeue`.
  - **Map (or Dictionary) ADT:** Operations include inserting, deleting, and searching for key-value pairs.

<br>

:::

<br>

::: {.lead}
##### Differences
:::

<br>

::: {.panel-tabset}

## 1. Abstraction

  - **Data Types:** The most basic level; concerned with how data is stored and what operations are allowed.
  - **Data Structures:** A step higher in abstraction; concerned with how data is organized and accessed.
  - **ADTs:** The highest level of abstraction; concerned with the operations and behavior of a data structure, abstracted away from the implementation details.

## 2. Purpose

  - **Data Types:** Define the type and nature of the data.
  - **Data Structures:** Provide a way to organize and manage data efficiently.
  - **ADTs:** Define a blueprint for data structures, focusing on what operations can be performed and what their expected behavior is.

## 3. Implementation

  - **Data Types:** Directly supported by the programming language.
  - **Data Structures:** Built using data types and can be complex.
  - **ADTs:** Can be implemented using various data structures; the choice of implementation depends on the specific needs like performance and memory usage.

## 4. Examples

  - **Data Types:** int, float, char, bool
  - **Data Structures:** Arrays, linked lists, trees, hash tables
  - **ADTs:** Stack, queue, list, set, map

<br>

:::

<br>

::: {.lead}
##### Array
:::

<br>

::: {.panel-tabset}

## 1. Description

  - An **array** is a collection of homogeneous elements stored in contiguous memory locations. Each element in the array can be accessed using its index.
  - **Python Equivalent:** `list`

## 2. Operations

  - **Access:** $O(1)$ — Direct access to elements via index. 
  - **Search:**
  
      - **Linear Search:** $O(n)$
      - **Binary Search:** $\text{O(log⁡ n)}$ *(only if the array is sorted)*
      
  - Insertion:
  
      - **At the End:** Append the element to the end of the array. If the array is full *(fixed size)*, you may need to resize it.
        - **Time Complexity:** $O(1)$ *(amortized if resizing is needed)*.
      - **At a Specific Index:** Shift elements to the right from the index to create space, then insert the new element.
        - **Time Complexity:** $O(n)$, where n is the number of elements after the insertion point.
        
  - Deletion:
  
      - **From the End:** Remove the last element.
        - **Time Complexity:** $O(1)$.
      - **From a Specific Index:** Shift elements to the left to fill the gap left by the removed element.
        - **Time Complexity:** $O(n)$, where n is the number of elements after the removal point.

## 3. Usage

  - **Use Cases:**
  
    - Suitable for scenarios where fast access to elements is required, and the size of the data set is known.

<br>

:::

<br>

::: {.lead}
##### Linked List
:::

<br>

::: {.panel-tabset}

## Description

  - A linked list is a linear collection of elements called nodes, where each node contains data and a reference *(or pointer)* to the next node in the sequence.
  - Python Equivalent: Custom class with Node and LinkedList classes

## Types

  - Singly Linked List: Each node points to the next node.
  - Doubly Linked List: Each node points to both the next and previous nodes.
  - Circular Linked List: The last node points back to the first node, forming a loop.

## Operations

  - **Access:** $O(n)$ — Requires traversal from the head to the desired node.
  - **Search:** $O(n)$ — Requires traversal to find the target element.
  
  - **Insertion:**
  
      - **At the Beginning (Singly/ Doubly Linked List):** Create a new node and adjust the head *(and possibly tail in a doubly linked list)*.
        - **Time Complexity:** $O(1)$.
      - **At the End (Singly Linked List):** Traverse to the last node, then insert the new node.
        - **Time Complexity:** $O(n)$.
      - **At a Specific Position:** Traverse to the position and insert the node, adjusting the pointers.
        - **Time Complexity:** $O(n)$.
        
  - **Deletion:**
  
      - **From the Beginning:** Adjust the head pointer to the next node.
        - **Time Complexity:** $O(1)$.
      - **From the End:** Traverse to the second last node, adjust its pointer to null.
        - **Time Complexity:** $O(n)$.
      - **From a Specific Position:** Traverse to the node before the one to remove, then adjust pointers to bypass the removed node.
        - **Time Complexity:** $O(n)$.

## Usage

  - **Use Cases:**
  
    - Useful when the size of the data set is unknown or when frequent insertions and deletions are required.

<br>

:::

<br>

::: {.lead}
##### Methods for Lists/Arrays
:::

<br>

::: {.panel-tabset}

## I ...

  - **append()**
    - **Description:** Adds an element to the end of the list.
    - **Syntax:** list.append(element)
  
**Example:**

```{python pyAppend}
fruits = ["apple", "banana"]
fruits.append("orange")
print(fruits)  # Output: ['apple', 'banana', 'orange']
```

  - **extend()**
    - **Description:** Extends the list by appending elements from another iterable *(e.g., another list)*.
    - **Syntax:** list.extend(iterable)

**Example:**

```{python pyExtend}
numbers = [1, 2, 3]
numbers.extend([4, 5])
print(numbers)  # Output: [1, 2, 3, 4, 5]
```

<br>

## II ...

  **insert()**
    - **Description:** Inserts an element at a specific index in the list.
    - **Syntax:** list.insert(index, element)

**Example:**

```{python pyInsert}
fruits = ["apple", "banana"]
fruits.insert(1, "orange")
print(fruits)  # Output: ['apple', 'orange', 'banana']
```

  - **remove()**
    - **Description:** Removes the first occurrence of a specified value from the list.
    - **Syntax:** list.remove(element)

**Example:**

```{python pyRemove}
fruits = ["apple", "banana", "orange"]
fruits.remove("banana")
print(fruits)  # Output: ['apple', 'orange']
```

<br>

## III ...

  - **pop()**
    - **Description:** Removes and returns the element at the specified index. If no index is specified, it removes and returns the last element.
    - **Syntax:** list.pop(index)

**Example:**

```{python pyPop}
fruits = ["apple", "banana", "orange"]
last_fruit = fruits.pop()
print(last_fruit)  # Output: 'orange'
print(fruits)      # Output: ['apple', 'banana']
```

  - **index()**
    - **Description:** Returns the index of the first occurrence of a specified value.
    - **Syntax:** list.index(element, start, end)

**Example:**

```{python pyIndex}
fruits = ["apple", "banana", "orange"]
index_of_banana = fruits.index("banana")
print(index_of_banana)  # Output: 1
```

<br>

## IV ...

  - **count()**
    - **Description:** Returns the number of occurrences of a specified value in the list.
    - **Syntax:** list.count(element)

**Example:**

```{python pyCount}
fruits = ["apple", "banana", "orange", "banana"]
count_of_banana = fruits.count("banana")
print(count_of_banana)  # Output: 2
```

  - **sort()**
    - **Description:** Sorts the elements of the list in ascending order *(or descending order if specified)* in place.
    - **Syntax:** list.sort(reverse=False)

**Example:**

```{python pySort}
numbers = [3, 1, 4, 1, 5, 9]
numbers.sort()
print(numbers)  # Output: [1, 1, 3, 4, 5, 9]
```

<br>

## V ...

  - **reverse()**
    - **Description:** Reverses the elements of the list in place.
    - **Syntax:** list.reverse()

**Example:**

```{python pythonReverse}
numbers = [1, 2, 3, 4]
numbers.reverse()
print(numbers)  # Output: [4, 3, 2, 1]
```

  - **copy()**
    - **Description:** Returns a shallow copy of the list.
    - **Syntax:** list.copy()

**Example:**

```{python pyCopy}
fruits = ["apple", "banana", "orange"]
fruits_copy = fruits.copy()
print(fruits_copy)  # Output: ['apple', 'banana', 'orange']
```

<br>

**Java ArrayList Methods**

  - `**add():**` Adds an element to the end of the list.
  - `**remove():**` Removes the first occurrence of a specified element.
  - `**get():**` Retrieves the element at a specified index.
  - `**set():**` Replaces the element at a specified index with a new element.
  - `**size():**` Returns the number of elements in the list.

<br>

:::

<br>

##### Record

  - A **Record** is a composite data structure used to store a collection of related fields, each with a specific name and data type. 
  - It is commonly used to model entities in databases and programming, allowing for structured and organized data storage. 
  - Records are versatile and can represent complex objects with multiple attributes, making them essential in many applications.
  - In Python, a **Record** is typically implemented as a **class**, a `namedtuple`, or a `dataclass`. Each of these provides a way to group multiple fields *(attributes)* together under one name, similar to how a record in other languages might work.

<br>

::: {.lead}
##### Stack
:::

<br>

::: {.panel-tabset}

## Description

  - A **stack** is a linear data structure that follows the Last-In-First-Out (LIFO) principle, meaning the last element added is the first one to be removed.
  - **Python Equivalent:** `list` or `collections.deque`

## Operations

  - **Push (Insertion):** $O(1)$ — Add an element to the top of the stack.
  - **Pop (Deletion):** $O(1)$ — Remove the top element from the stack.
  - **Peek/Top:** $O(1)$ — View the top element without removing it.
  - **IsEmpty:** $O(1)$ — Check if the stack is empty.

## Usage

  - **Use Cases:**
  
    - Used in expression evaluation, backtracking algorithms, undo mechanisms in applications, and for maintaining function call stacks in recursion.

<br>

:::

<br>

::: {.lead}
##### Bag
:::

<br>

::: {.panel-tabset}

## Description

  - A **Bag** *(also known as a multiset)* is a simple data structure that allows for storing a collection of elements where duplicate elements are allowed. Unlike a set, which requires all elements to be unique, a bag can contain multiple occurrences of the same element.
  - **Python Equivalent:** `collections.Counter`

## Operations

  - **Add (**`add`**)** — Adds an element to the bag.
    - Simply add the element, usually at the end or beginning, depending on the implementation *(array, linked list, etc.)*.
      - **Time Complexity:** $O(1)$.
  - **Check Membership (**`contains`**)** — Checks if an element is in the bag.
  - **Count Occurrences (**`count`**)** — Counts how many times an element appears in the bag.
  - **Remove (**`remove`**)** — Removes one occurrence of an element from the bag.
    - Typically, bags do not have a direct remove operation unless implemented, in which case:
      - Remove Specific Element: Find the element and remove it, adjusting structure accordingly.
        - Time Complexity: $O(n)$.
  - **Get Size (**`size`**)** — Returns the total number of elements in the bag, including duplicates.

## Usage

  - **Use Cases:**
  
    - A bag is useful when you need to count the number of occurrences of items, such as counting words in a document.

<br>

:::

<br>

::: {.lead}
##### Queue
:::

<br>

::: {.panel-tabset}

## Description

  - A **queue** is a linear data structure that follows the First-In-First-Out (FIFO) principle, meaning the first element added is the first one to be removed.
  - **Python Equivalent:** `collections.deque` or `queue.Queue`

## Operations

  - **Enqueue (Insertion/Push):** $O(1)$ — Add an element to the end of the queue.
  - **Dequeue (Deletion/Pop):** $O(1)$ — Remove the front element from the queue.
  - **Front/Peek:** $O(1)$ — View the front element without removing it.
  - **IsEmpty:** $O(1)$ — Check if the queue is empty.

## Usage

  - **Use Cases:**
  
    - Useful in scheduling processes, managing tasks in order, breadth-first search (BFS) algorithms, and handling requests in servers.

<br>

:::

<br>

::: {.lead}
##### Deque
:::

<br>

::: {.panel-tabset}

## Description

  - A **deque** is a linear data structure that allows insertion and deletion of elements from both the front and rear ends.
  - **Python Equivalent:** `collections.deque`

## Operations

  - **InsertFront:** $O(1)$ — Add an element to the front.
  - **InsertLast:** $O(1)$ — Add an element to the end.
  - **DeleteFront:** $O(1)$ — Remove an element from the front.
  - **DeleteLast:** $O(1)$ — Remove an element from the end.
  - **PeekFront:** $O(1)$ — View the front element.
  - **PeekLast:** $O(1)$ — View the last element.
  - **IsEmpty:** $O(1)$ — Check if the deque is empty.

## Usage

  - **Use Cases:**
  
    - Useful in scenarios requiring access from both ends, such as the implementation of both stacks and queues, task scheduling, and sliding window algorithms.

<br>

:::

<br>

::: {.callout-note title="Hash Table" collapse="true" appearance="simple" icon="false" style="border-left:none; border-right:none; border-top:none; border-bottom:none;"}

###### Descriptions

  - A hash table is a data structure that maps keys to values using a hash function, which transforms the key into an index in an array.
  - **Python Equivalent:** `dict`

<br>

###### Operations

  - **Insert:** $O(1)$ — Map a key to a value by computing its hash.
  - **Search:** $O(1)$ — Retrieve the value associated with a given key.
  - **Delete:** $O(1)$ — Remove the key-value pair from the table.

<br>

###### Usage

  - **Use Cases:**
  
    - Highly efficient for scenarios requiring fast lookups, such as databases, caches, and dictionaries.

<br>

###### Collisions

  - **Handling Collisions:**
  
    - **Chaining:** Store multiple elements at the same index using a linked list.
    - **Open Addressing (Probing):** Find another open slot using techniques like linear probing, quadratic probing, or double hashing.

<br>

###### Hashing

  - **Hashing**
  
    - **Hashing** is the process of mapping keys to indices in an array using a hash function. It ensures efficient data retrieval by minimizing the number of comparisons required.

  **Hash Function**
    - A **hash function** is a function that converts input *(key)* into a fixed-size value, typically an integer, which serves as an index in the hash table array.
     - A good hash function has the following properties:
     
      - **Deterministic:** The same key always produces the same hash value.
      - **Uniform Distribution:** The hash values should be distributed uniformly across the array to minimize collisions.
      - **Fast Computation:** The function should be quick to compute.

  - **Hash Keys**
  
    - **Hash Key:** The key for which the hash function generates an index.
    - **Hash Value:** The integer index produced by the hash function.

<br>


###### **How to perform hash function on calculator**

To determine where the data associated with the last 4 digits of the Social Security number *('2023')* will be stored in the array, follow these steps:

  **1.** Understand the Hash Function: The problem specifies a hash function given by key % 1009. Here, % denotes the modulus operation. This operation returns the remainder of the division of key by 1009.

  **2.** Determine the Key: The last 4 digits of the Social Security number are '2023'. Therefore, the key you're interested in is 2023.

  **3.** Apply the Hash Function: Use the hash function to find the index in the array where the data should be stored. Specifically, you need to compute:

    a. Key / Hash - (whole number in result) * Hash
      i. For this example
        1. $(2023 / 1009) = 2.004955401$
        2. $2.004955401 - 2 = 0.004955401$
        3. $0.004955401 * 1009 = 5$

  **4.** By calculating the remainder, you will find the index in the array soc where the data associated with the key '2023' will be stored.

    a. soc[5]

<br>

:::

<br>

::: {.lead}
##### Trees
:::

<br>

::: {.callout-note title="Tree" collapse="true" appearance="simple" icon="false" style="border-left:none; border-right:none; border-top:none; border-bottom:none;"}

<br>

###### Description

  - A **tree** is a hierarchical data structure composed of nodes, where each node contains a value and references to child nodes. The top node is called the root, and nodes with no children are called leaves.
  - **Python Equivalent:** Custom class with `Node` and `Tree` classes

<br>

###### Types

  - **Binary Tree:** Each node has at most two children *(left and right)*.
  - **Binary Search Tree (BST):** A binary tree where the left child contains values less than the parent, and the right child contains values greater than the parent.
  - **AVL Tree:** A self-balancing binary search tree.
  - **Red-Black Tree:** Another type of self-balancing binary search tree.
  - **B-Tree:** A self-balancing tree that maintains sorted data and allows searches, sequential access, insertions, and deletions in logarithmic time.

<br>

###### Forms

  - **Full Binary Tree**  — Definition: A binary tree in which every node has either 0 or 2 children.
  - **Complete Binary Tree** — Definition: A binary tree in which all levels are completely filled except possibly the last level, which is filled from left to right.
  - **Perfect Binary Tree** — Definition: A binary tree in which all interior nodes have two children and all leaves are at the same level.
  - **Balanced Binary Tree** — Definition: A binary tree where the height of the left and right subtrees of any node differ by at most one.

<br>

###### Operations

  - **Insertion:** $\text{O(log ⁡n)}$ for balanced trees like AVL or Red-Black Trees; $O(n)$ for unbalanced trees.

  - **Deletion:** $\text{O(log ⁡n)}$ for balanced trees; $O(n)$ for unbalanced trees.
    - **BST:** Find the node to be removed, then:
        - **No Children:** Just remove the node.
        - **One Child:** Bypass the node and link its parent directly to its child.
        - **Two Children:** Find the in-order predecessor *(or successor)*, swap values, and remove the predecessor *(or successor)* node.
        - **Time Complexity:** $\text{O(log n)}$ on average, $O(n)$ in the worst case.

  - **Search:** $\text{O(log ⁡n)}$  for balanced trees; $O(n)$ for unbalanced trees.

  - **Traversal:**

    - **In-order:** $O(n)$ — Left, Root, Right *(used in BSTs to get sorted order)*.
    - **Pre-order:** $O(n)$ — Root, Left, Right.
    - **Post-order:** $O(n)$ — Left, Right, Root.
    - **Level-order:** $O(n)$— Traverse nodes level by level.

<br>

###### Usage

  - **Use Cases:**
  
    - Suitable for hierarchical data *(like file systems)*, database indexing, and scenarios requiring sorted data with dynamic insertion and deletion.

<br>

:::

<br>

::: {.lead}
##### Adelson-Velsky and Landis Trees
:::

<br>

::: {.callout-note title="AVL Trees" collapse="true" appearance="simple" icon="false" style="border-left:none; border-right:none; border-top:none; border-bottom:none;"}

<br>

###### Properties

  - An AVL tree is a binary search tree in which the height of the two child subtrees of any node differs by at most one.
  - If at any time during insertion or deletion this condition is violated, the tree is rebalanced through rotations.

<br>

###### Balancing

  - **Single Rotation:** Used when a node is inserted into the left subtree of the **left child** or the `right subtree` of the **right child**.
  - **Double Rotation:** Used when a node is inserted into the left subtree of the right child or the `right subtree` of the **left child**.

<br>

###### Time

  - **Time Complexity:**
  
    - **Search**, **Insertion**, **Deletion:** $\text{O(log n)}$, where n is the number of nodes.

<br>

###### Usage

  - **Use Cases:**
  
    - AVL trees are well-suited for applications where frequent insertions and deletions occur, and maintaining strict balance is important.

<br>

:::

<br>

::: {.lead}
##### Red-Black
:::

<br>

::: {.callout-note title="Red-Black Trees" collapse="true" appearance="simple" icon="false" style="border-left:none; border-right:none; border-top:none; border-bottom:none;"}

<br>

###### Properties

  - A Red-Black tree is a binary search tree with an extra bit of storage per node: its color, which can be either red or black.
  - The tree satisfies several properties:
  
      1. Every node is either red or black.
      2. The root is always black.
      3. All leaves (NIL nodes) are black.
      4. Red nodes cannot have red children *(no two red nodes can be adjacent)*.
      5. Every path from a node to its descendant NIL nodes must have the same number of black nodes *(black height)*.

<br>

###### Balancing

  - The tree is kept balanced by performing rotations and color changes during insertions and deletions.

<br>

###### Time

  - **Time Complexity:**
  
    - **Search**, **Insertion**, **Deletion:** $\text{O(log n)}$.

<br>

###### Usage

  - **Use Cases:**
  
    - Red-Black trees are used in many systems, such as the Linux kernel's process scheduling and in standard libraries like C++'s **std::map** and **std::set**, due to their relatively simple implementation and good performance.

<br>

:::

<br>

::: {.lead}
##### Balanced Tree
:::

<br>

::: {.callout-note title="B-Trees" collapse="true" appearance="simple" icon="false" style="border-left:none; border-right:none; border-top:none; border-bottom:none;"}

<br>

###### Properties

  - A B-tree is a self-balancing search tree in which nodes can have more than two children. It's commonly used in databases and file systems.
  - A B-tree of order `m` can have at most `m-1` keys and `m` children.
  - All leaf nodes are at the same depth, and internal nodes act as a guide to direct searches.

<br>

###### Balancing

  - Balancing in B-trees is achieved through splitting and merging nodes.
  - When a node in a B-tree becomes too full *(i.e., has more than m-1 keys)*, it is split into two nodes, and the middle key is pushed up to the parent node.
  - When a node has too few keys, it may borrow keys from its neighbors or merge with a neighboring node.

<br>

###### Time

  - **Time Complexity:**
  
    - **Search**, **Insertion**, **Deletion:** $\text{O(log n)}$.

<br>

###### Usage

  - **Use Cases:**
  
    - B-trees are particularly effective for systems that read and write large blocks of data, such as databases and file systems, where minimizing disk I/O operations is critical.

<br>

:::

<br>

---

::: {.lead}
###### **Wrap-up**
:::

<br>

::: {.callout-note title="Comparison Summary" collapse="true" appearance="simple" icon="false" style="border-left:none; border-right:none; border-top:none; border-bottom:none;"}

  - **AVL Trees:** Strictly balanced, more rotations, better for search-heavy applications.
  - **Red-Black Trees:** Looser balancing, fewer rotations, generally faster insertions and deletions.
  - **B-Trees:** Optimized for storage systems, used in databases, and file systems for handling large volumes of data.

:::

::: {.callout-note title="Tree Traversal" collapse="true" appearance="simple" icon="false" style="border-left:none; border-right:none; border-top:none; border-bottom:none;"}

  - *Pre-Order Traversal (NLR)* - Node, Left, Right
  - *In-Order Traversal (LNR)* - Left, Node, Right
  - *Post-Order Traversal (LRN)* - Left, Right, Node

:::

<br>

---

<br>

::: {.lead}
##### Pre-Order Traversal (NLR)
:::

<br>

::: {.panel-tabset}

## Order:

::: {.text-center}
**N**ode → **L**eft → **R**ight
:::

## Process:

  - Visit the **Node** first.
  - Then recursively visit the **Left** subtree.
  - Finally, visit the **Right** subtree.

## Examples:

If you have a tree like this:

```{text treeNLR}
    A
   / \
  B   C
 / \
D   E
```

  - The Pre-Order traversal would be: **A B D E C**

:::

<br>

::: {.lead}
##### In-Order Traversal (LNR)
:::

<br>

::: {.panel-tabset}

## Order:

::: {.text-center}
**L**eft → **N**ode → **R**ight
:::

## Process:

  - Recursively visit the **Left** subtree.
  - Then visit the **Node**.
  - Finally, visit the **Right** subtree.

## Example:

For the same tree:

```{text treeLNR}
    A
   / \
  B   C
 / \
D   E
```

  - The In-Order traversal would be: **D B E A C**

:::

<br>

::: {.lead}
##### Post-Order Traversal (LRN)
:::

<br>

::: {.panel-tabset}

## Order:

::: {.text-center}
**L**eft → **R**ight → **N**ode
:::

## Process:

  - Recursively visit the **Left** subtree.
  - Then visit the **Right** subtree.
  - Finally, visit the **Node**.

## Examples:

For the same tree:

```{text treeLRN}
    A
   / \
  B   C
 / \
D   E
```

  - The Post-Order traversal would be: **D E B C A**

:::

<br>

---

::: {.lead}
###### **Wrap-up**
:::

<br>

::: {.callout-note title="How to Remember Them:" collapse="true" appearance="simple" icon="false" style="border-left:none; border-right:none; border-top:none; border-bottom:none;"}

  - **Pre-Order (NLR):** Think of "Pre" as "before" - you process the Node before anything else.
  - **In-Order (LNR):** "In" implies "in between" - the Node is processed in between the Left and Right subtrees.
  - **Post-Order (LRN):** "Post" means "after" - the Node is processed after the subtrees.

:::

::: {.callout-note title="Visual Mnemonic:" collapse="true" appearance="simple" icon="false" style="border-left:none; border-right:none; border-top:none; border-bottom:none;"}

  - **Pre-Order:** Imagine starting at the root and touching it first.
  - **In-Order:** Imagine walking along the edge of the tree, touching the left side first, then the root, and then the right side.
  - **Post-Order:** Imagine leaving the tree by processing the children before the root.

:::

<br>

---

<br>

::: {.lead}
##### Heap
:::

<br>

::: {.panel-tabset}

## Description:

  - A **heap** is a specialized tree-based data structure that satisfies the heap property. In a **max-heap**, the parent node is always greater than or equal to its children; in a **min-heap**, the parent node is always less than or equal to its children.
  - **Python Equivalent:** `heapq` *(min-heap by default)*

## Operations:

  - **Insert:** $\text{O(log ⁡n)}$ — Add a new element and adjust the heap to maintain the heap property.
    - **Add the Element at the End:**
      - Insert the new element at the end of the heap *(i.e., the next available leaf node)*.
    - **Heapify Up (Percolate Up):**
      - Compare the inserted element with its parent node.
      - If the heap property *(Min-Heap or Max-Heap)* is violated *(e.g., in a Min-Heap, if the new element is smaller than its parent)*, swap the element with its parent.
      - Repeat the process until the heap property is restored or the element becomes the root.
  - **DeleteMax/Min:** $\text{O(log ⁡n)}$ — Remove the root *(max or min)* and adjust the heap.
    - **Remove the Root Element:**
      - The root element of the heap *(the smallest element in a Min-Heap or the largest in a Max-Heap)* is removed. This is because the root element has the highest priority.
    - **Replace the Root with the Last Element:**
      - Move the last element in the heap *(the rightmost leaf node)* to the root position.
    - **Heapify Down (Percolate Down):**
      - Compare the new root element with its children.
      - If the heap property is violated *(e.g., in a Min-Heap, if the root is greater than any of its children)*, swap the root with the smallest child *(in a Max-Heap, swap with the largest child)*.
      - Repeat the process down the tree until the heap property is restored.
  - **PeekMax/Min:** $O(1)$ — Access the root element.
  - **Heapify:** $O(n)$ — Convert an unsorted array into a heap.

## Use Cases:

  - Often used to implement priority queues, scheduling algorithms, and for efficient sorting *(Heap Sort)*.

:::

<br>

---

::: {.lead}
###### **Steps To Figure Out The Index Of Child Nodes in Heaps**
:::

<br>

::: {.callout-note title="The Index Of The Right Child Of An Item In A Heap" collapse="true" appearance="simple" icon="false" style="border-left:none; border-right:none; border-top:none; border-bottom:none;"}

**heapList = [22, 33, 44, 55, 66]**

  - Identify the index of the item:
      - Find the index of the item 22 in the heap list. Let's call this index $i$.

  - Use the formula for the right child:
      - In a binary heap, if an element is at index $i$, the index of its right child is given by the formula: $2i + 2$.

  - Apply the formula:
      - Substitute the value of $i$ obtained from step 1 into the formula $2i+2$.

  - Verify the index:
      - Ensure the calculated index falls within the bounds of the heap list.

:::

::: {.callout-note title="The Index Of The Left Child Of An Item In A Heap" collapse="true" appearance="simple" icon="false" style="border-left:none; border-right:none; border-top:none; border-bottom:none;"}

**heapList = [22, 33, 44, 55, 66]**

  - Identify the index of the item:
      - Find the index of the item in the heap list. Let’s call this index $i$.

  - Use the formula for the left child:
      - In a binary heap, if an element is at index $i$, the index of its left child is given by the formula: $2i+1$.

  - Apply the formula:
      - Substitute the value of $iii$ obtained from step 1 into the formula $2i+1$.

  - Verify the index:
      - Ensure the calculated index falls within the bounds of the heap list.

:::

<br>

---

<br>

::: {.lead}
##### Set
:::

<br>

::: {.panel-tabset}

## Description:

  - A **set** is an abstract data structure that stores unique elements, with no specific order. It supports operations that allow the management of unique collections of data.
  - **Python Equivalent:** `set`

## Operations:

  - **Insert:** $O(1)$ on average — Add a new element if it’s not already present.
  - **Delete:** $O(1)$ on average — Remove an element if it exists.
  - **Search:** $O(1)$ on average — Check if an element is present in the set.
  - **Union:** $O(n)$ — Combine elements from two sets.
  - **Intersection:** $O(n)$ — Get common elements between two sets.
  - **Difference:** $O(n)$ — Get elements present in one set but not the other.

## Usage:

  - Useful in situations requiring the management of unique elements, such as maintaining a list of unique IDs, handling membership checks, and performing mathematical set operations.

:::

<br>

::: {.lead}
##### Graph
:::

<br>

::: {.panel-tabset}

## Description:

  - A **graph** is a collection of nodes *(vertices)* connected by edges. Graphs can be **directed** *(edges have a direction)* or **undirected** *(edges do not have a direction)*.
  - **Python Equivalent:** `dict` of lists, `collections.defaultdict`, or custom class

## Types:

  - **Directed Graph (Digraph):** All edges have a direction.
  - **Undirected Graph:** Edges do not have direction.
  - **Weighted Graph:** Edges have weights representing costs or distances.
  - **Unweighted Graph:** Edges have no weights.

## Operations:

  - **Add Vertex:** $O(1)$ — Add a new node.
    - **Vertex:** Simply add the vertex to the vertex set.
      - **Time Complexity:** $O(1)$.

  - **Add Edge:** $O(1)$ — Add a connection between two nodes.
    - **Edge:** Add an edge by connecting two vertices, updating adjacency lists or matrices.
      - **Time Complexity:** $O(1)$ for adjacency list, $O(1)$ for adjacency matrix.

  - **Remove Vertex:** $O(V+E)$ — Remove a node and its associated edges.
    - **Vertex:** Remove the vertex and all associated edges.
      - **Time Complexity:** $\text{O(V + E)}$ in an adjacency list, $O(V^2)$ in an adjacency matrix, where V is the number of vertices and E is the number of edges.

  - **Remove Edge:** $O(1)$ — Remove a connection between two nodes.
    - **Edge:** Remove the edge between two vertices.
      - **Time Complexity:** $O(1)$ for adjacency list, $O(1)$ for adjacency matrix.

  - **Search:**
    - **Depth-First Search (DFS):** $O(V+E)$ — Explore as far as possible along each branch before backtracking.
    - **Breadth-First Search (BFS):** $O(V+E)$— Explore all neighbors of a node before moving to the next level.

## Usage:

-

:::

<br>

::: {.lead}
##### Path Algorithms
:::

<br>

::: {.callout-note title="Dijkstra's Shortest Path Algorithm" collapse="true" appearance="simple" icon="false" style="border-left:none; border-right:none; border-top:none; border-bottom:none;"}

  - **Overview:**

    - **Type:** Greedy algorithm.
    - **Applicability:** Works on graphs with non-negative edge weights.
    - **Time Complexity:** $O(V2)$ for the simplest implementation, $\text{O(V log V+E)}$ with a priority queue *(using a binary heap)*, where $V$ is the number of vertices and $E$ is the number of edges.
    - **Functionality:** It finds the shortest path from a single source vertex to all other vertices in a graph by iteratively selecting the vertex with the smallest known distance, updating the distances to its neighbors, and marking it as visited.

:::

::: {.callout-note title="Bellman-Ford Shortest Path Algorithm" collapse="true" appearance="simple" icon="false" style="border-left:none; border-right:none; border-top:none; border-bottom:none;"}

  - **Overview:**

    - **Type:** Dynamic programming algorithm.
    - **Applicability:** Works on graphs with negative edge weights and can detect negative weight cycles.
    - **Time Complexity:** $O(V*E)$, where $V$ is the number of vertices and $E$ is the number of edges.
    - **Functionality:** It calculates the shortest path from a single source vertex to all other vertices by relaxing all edges repeatedly over $V−1$ iterations.

:::

::: {.callout-note title="Differences Between Dijkstra and Bellman-Ford" collapse="true" appearance="simple" icon="false" style="border-left:none; border-right:none; border-top:none; border-bottom:none;"}

| Feature Dijkstra's       | Algorithm Bellman-Ford               | Algorithm                   |
| -------------------------|-------------------------------------:|:---------------------------:|
| Edge Weights             | Non-negative edge weights only       | Handles negative edge weights |
| Negative Cycle Detection | Cannot detect negative cycles        | Can detect negative cycles |
| Time Complexity          | $O(V2)$ for the simplest implementation, $\text{O(V log V+E)}$ with a priority queue *(using a binary heap)*, where $V$ is the number of vertices and E is the number of edges. | $O(V*E)$, where $V$ is the number of vertices and $E$ is the number of edges. |
| Graph Type               | Directed or undirected with non-negative weights| Directed or undirected with any weights, including negative|
| Algorithm Type           | Greedy                               | Dynamic programming          |
| Use Case                 | Faster for graphs with non-negative weights   | More general, can be used when negative weights are present    |

: Comparison chart

:::

<br>

---

::: {.lead}
###### **Summary**
:::

<br>

::: {.callout-note title="Summary of ADTs" collapse="true" appearance="simple" icon="false" style="border-left:none; border-right:none; border-top:none; border-bottom:none;"}

  - **Array:** Fixed-size, contiguous memory; fast access by index.
  - **Linked List:** Dynamic size; efficient insertions and deletions.
  - **Stack:** LIFO order; used in recursion, undo operations.
  - **Queue:** FIFO order; used in scheduling, buffering.
  - **Deque:** Double-ended queue; flexible insertions/deletions.
  - **Hash Table:** Key-value pairs; fast lookups and inserts.
  - **Tree:** Hierarchical structure; efficient searches and sorted data.
  - **Heap:** Binary tree for priority queues; fast access to max/min.
  - **Set:** Unique elements; fast membership checks.
  - **Graph:** Nodes and edges; used in networks and relationship modeling.

:::

<br>

---

<br>

::: {.lead}
##### Key Operations in a Dictionary/Map
:::

<br>

A **Dictionary** (in Python) or **Map** (in many other programming languages) is a data 
structure that stores key-value pairs, where each unique key maps to a specific 
value. Dictionaries/Maps provide efficient insertion, deletion, and lookup 
operations, typically in $O(1)$ time on average due to the underlying hash table 
implementation.

<br>

###### Insertion

Description: Adding a new key-value pair to the dictionary.

Syntax:

```{text insertionSyntax}
dictionary[key] = value
```


Example:

```{text insertionExample}
phone_book = {}
phone_book['Alice'] = '555-1234'
```

This adds the key `'Alice'` with the value `'555-1234'` to the `phone_book` dictionary.

<br>

###### Lookup (Access)

Description: Retrieving the value associated with a given key.

Syntax:
```{text lookupValue}
value = dictionary[key]
```


Example:
```{text lookupVariable}
alice_number = phone_book['Alice']
```

This retrieves the value associated with `'Alice'`, which is `'555-1234'`.

<br>

###### Deletion

Description: Removing a key-value pair from the dictionary.

Syntax:
```{text deletionKey}
del dictionary[key]
```


Example:
```{text deletionCode}
del phone_book['Alice']
```

This removes the `'Alice'` entry from the `phone_book` dictionary.

<br>

###### Update

Description: Updating the value associated with a given key.

Syntax:
```{text updateValue}
dictionary[key] = new_value
```


Example:
```{text updateVariable}
phone_book['Alice'] = '555-5678'
```


This updates `'Alice's` number to `'555-5678'`.

<br>

###### Check Existence

Description: Checking if a key exists in the dictionary.

Syntax:
```{text checkKey}
key in dictionary
```

Example:
```{text checkLogic}
if 'Alice' in phone_book:
    print("Alice is in the phone book")
```

This checks if `'Alice'` is a key in `phone_book`.

<br>

###### Iteration

Description: Iterating over keys, values, or key-value pairs in the dictionary.

Syntax:
```{text iterationKey}
Iterate over keys
for key in dictionary:
    print(key)
```

Iterate over values
```{text iterationValue}
for value in dictionary.values():
    print(value)
```

Iterate over key-value pairs
```{text iterationPair}
for key, value in dictionary.items():
    print(f"{key}: {value}")
```

Example:
```{text iterationName}
for name, number in phone_book.items():
    print(f"{name}: {number}")
```

This prints all key-value pairs in `phone_book`.

<br>

###### Get Method

Description: Retrieving the value associated with a given key, with an optional 
default if the key doesn’t exist.

Syntax:
```{text getValue}
value = dictionary.get(key, default_value)
```

Example:
```{text getVariable}
bob_number = phone_book.get('Bob', 'Not Found')
```

If `'Bob'` is not in `phone_book`, `bob_number` will be `'Not Found'`.

<br>

###### Length

Description: Getting the number of key-value pairs in the dictionary.

Syntax:
```{text lengthValue}
length = len(dictionary)
```

Example:
```{text lengthVariable}
num_contacts = len(phone_book)
```

This returns the number of entries in `phone_book`.

<br>

###### Clearing

Description: Removing all key-value pairs from the dictionary.

Syntax:
```{text clearingFunction}
dictionary.clear()
```

Example:
```{text clearingVariable}
phone_book.clear()
```

This removes all entries from `phone_book`, making it an empty dictionary.

<br>

###### Copying

Description: Creating a shallow copy of the dictionary.

Syntax:
```{text copyingValue}
new_dictionary = dictionary.copy()
```

Example:
```{text copyingVariable}
backup_phone_book = phone_book.copy()
```

This creates a copy of `phone_book` called `backup_phone_book`.

<br>

###### Pop

Description: Removing a key from the dictionary and returning its value.

Syntax:
```{text popValue}
value = dictionary.pop(key, default_value)
```

Example:
```{text popFunction}
removed_number = phone_book.pop('Alice', 'Not Found')
```

Removes `'Alice'` from `phone_book` and returns her number. If `'Alice'` is not found, it returns `'Not Found'`.

<br>

###### Popitem

Description: Removes and returns an arbitrary key-value pair as a tuple *(key, value)*.

Syntax
```{text pitemPair}
key, value = dictionary.popitem()
```

Example
```{text pitemVariable}
last_entry = phone_book.popitem()
```

Removes and returns the last inserted key-value pair in the dictionary.

<br>

###### Setdefault

Description: Returns the value of a key if it exists; otherwise, inserts the key with a specified value and returns that value.

Syntax:
```{text defaultValue}
value = dictionary.setdefault(key, default_value)
```

Example:
```{text defaultVariable}
alice_number = phone_book.setdefault('Alice', '555-0000')
```

If `'Alice'` is in `phone_book`, it returns her number. Otherwise, it adds `'Alice': '555-0000'` to phone_book and returns `'555-0000'`.

<br>

:::

<br>

## Applies Algorithms - (40%)
